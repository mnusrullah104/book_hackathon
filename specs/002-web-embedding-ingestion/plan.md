# Implementation Plan: Website URL Embedding Ingestion Pipeline

**Branch**: `002-web-embedding-ingestion` | **Date**: 2025-12-24 | **Spec**: [spec.md](spec.md)
**Input**: Feature specification from `/specs/002-web-embedding-ingestion/spec.md`

## Summary

Build a production-ready Python pipeline that ingests website URLs, extracts text content, generates semantic embeddings using Cohere API, and stores them in Qdrant vector database. The pipeline supports batch processing, configurable chunking strategies, automatic retry logic, and provides a simple programmatic interface for FastAPI integration. Per user requirements, implementation will be in a single `backend/main.py` file using UV package manager.

## Technical Context

**Language/Version**: Python 3.9+
**Primary Dependencies**: BeautifulSoup4 (HTML parsing), aiohttp (async HTTP), Cohere SDK (embeddings), Qdrant Client (vector storage), LangChain (chunking), tiktoken (tokenization)
**Storage**: Qdrant Cloud (vector database), no SQL database required
**Testing**: pytest with async support (pytest-asyncio)
**Target Platform**: Linux/macOS/Windows (cross-platform Python)
**Project Type**: Single-file backend module (backend/main.py)
**Performance Goals**: Process 100 URLs with 95%+ success rate, 2-5 seconds per URL average
**Constraints**: Cohere free tier rate limits (10 req/min), single-file implementation, async-based concurrency
**Scale/Scope**: MVP handles 1-1000 URLs per batch, optimized for technical documentation ingestion

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

### Compliance Review

| Principle | Status | Notes |
|-----------|--------|-------|
| **Spec-first development** | ✅ PASS | Formal spec created, all requirements documented |
| **Technical accuracy** | ✅ PASS | Cohere/Qdrant free tiers specified, reproducible setup |
| **Clarity for developers** | ✅ PASS | Python interface with examples, quickstart guide provided |
| **AI-native architecture** | ✅ PASS | RAG pipeline with embeddings and vector DB |
| **End-to-end transparency** | ✅ PASS | All steps logged, env vars documented |
| **Modular content** | ✅ PASS | Single-file implementation, production-quality only |

**Verdict**: ✅ All gates passed. No constitution violations.

## Project Structure

### Documentation (this feature)

```text
specs/002-web-embedding-ingestion/
├── spec.md              # Feature specification (4 user stories, 15 requirements)
├── plan.md              # This file - implementation plan
├── research.md          # Technical decisions (7 research areas resolved)
├── data-model.md        # Entity definitions (5 entities + Qdrant schema)
├── quickstart.md        # Setup guide (7 steps, 15-30 min completion)
├── contracts/
│   └── python-interface.md  # Public API contract with examples
└── checklists/
    └── requirements.md  # Spec quality validation (12/12 passed)
```

### Source Code (repository root)

Per user requirements: **Single-file implementation in backend/main.py**

```text
backend/
├── main.py              # Complete ingestion pipeline (all logic in one file)
├── .env                 # Environment variables (API keys, config)
├── .env.example         # Template for environment setup
├── requirements.txt     # Python dependencies (generated by UV)
└── README.md            # Integration instructions

# Project root files
.gitignore               # Ignore .env, __pycache__, etc.
```

**Structure Decision**: User explicitly requested single-file implementation (`backend/main.py`) for simplicity. All functionality (fetch, parse, chunk, embed, store) will be in one file organized as:

1. **Imports & Config** (top of file)
2. **Data Classes** (IngestionConfig, IngestionResult, FailedURL)
3. **Helper Functions** (fetch_url, extract_text, chunk_text, generate_embeddings, store_in_qdrant)
4. **Main Pipeline** (async main() function orchestrating full flow)
5. **Entry Point** (if __name__ == "__main__" block)

This violates typical "modular" design but matches user's explicit request for single-file simplicity.

## Complexity Tracking

> **No constitution violations - section left empty per instructions.**

## Design Decisions

### 1. Single-File Architecture

**Decision**: Implement entire pipeline in `backend/main.py` (~500-800 lines)

**Rationale** (from user requirements):
- User explicitly requested: "create a single main.py containing all ingestion logic"
- Simplifies deployment (one file to copy/import)
- Appropriate for focused utility (ingestion only, no retrieval/ranking)
- FastAPI integration via import: `from backend.main import main`

**Trade-offs**:
- ✅ **Pro**: Simplest possible deployment, no module structure needed
- ✅ **Pro**: Easy to understand flow (linear reading of file)
- ❌ **Con**: Less testable (no unit tests for individual functions without refactor)
- ❌ **Con**: Harder to extend if adding retrieval/ranking later

**Mitigation**: Functions within main.py will have clear docstrings and logical separation (comments marking sections).

---

### 2. UV Package Manager

**Decision**: Use UV for dependency management instead of pip

**Rationale** (from user requirements):
- User specified: "initialize project using UV package manager"
- UV is 10-100x faster than pip for dependency resolution
- Generates compatible `requirements.txt` for non-UV users
- No breaking changes to project structure (drop-in pip replacement)

**Commands**:
```bash
uv init                    # Initialize project
uv pip install <package>   # Install dependencies
uv pip freeze > requirements.txt  # Lock dependencies
```

---

### 3. Async Architecture

**Decision**: Use `asyncio` for pipeline execution

**Rationale**:
- I/O-bound workload (HTTP fetches, API calls)
- 10x throughput improvement vs. synchronous
- Native Python stdlib (no external runtime needed)
- Rate limit control via `asyncio.Semaphore`

**Pattern**:
```python
async def main(urls: list[str], ...) -> IngestionResult:
    async with aiohttp.ClientSession() as session:
        tasks = [process_url(url, session) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
    return aggregate_results(results)
```

---

### 4. Configuration Strategy

**Decision**: Environment variables + function parameters (no config file for MVP)

**Rationale**:
- **Secrets** (COHERE_API_KEY, QDRANT_API_KEY): Must use env vars (never hardcoded)
- **Operational params** (chunk_size, batch_size): Function parameters with defaults
- **Fallback chain**: Function param → env var → hardcoded default

**Example**:
```python
async def main(
    urls: list[str],
    cohere_api_key: str | None = None,  # Defaults to os.getenv("COHERE_API_KEY")
    chunk_size: int = 512,               # Default, overridable
    ...
):
    api_key = cohere_api_key or os.getenv("COHERE_API_KEY")
    if not api_key:
        raise ConfigurationError("COHERE_API_KEY not provided")
```

---

### 5. Error Handling Philosophy

**Decision**: Fail-fast on configuration, graceful degradation on runtime errors

**Rationale**:
- **Configuration errors** (missing API keys): Fail immediately with clear message
- **Runtime errors** (network timeout, rate limit): Retry with backoff, log failure, continue batch
- **Validation errors** (invalid chunk_size): Fail immediately before processing

**Categories**:
- **Transient**: 429 rate limit, 500 server error, network timeout → **Retry**
- **Permanent**: 401 auth failed, 404 not found, invalid URL → **Log & skip**
- **Critical**: Missing API key, Qdrant unreachable → **Fail entire batch**

---

## Implementation Roadmap

Per `/sp.plan` command scope, this plan defines WHAT to build, not HOW. The `/sp.tasks` command will break this into atomic implementation tasks.

### Phase 1: Core Pipeline (User Story 1 - P1)

**Goal**: Single URL ingestion end-to-end

**Components**:
1. **URL Fetching**: aiohttp GET request, handle HTTP errors
2. **Text Extraction**: BeautifulSoup HTML parsing, tag removal
3. **Chunking**: LangChain RecursiveCharacterTextSplitter (512 tokens, 50 overlap)
4. **Embedding**: Cohere API call (embed-english-v3.0, batch API)
5. **Storage**: Qdrant upsert with metadata payload
6. **Result Tracking**: IngestionResult with counts and timing

**Success Criteria**: Test with https://docs.python.org/3/tutorial/introduction.html, verify 5+ chunks stored in Qdrant

---

### Phase 2: Batch Processing (User Story 2 - P2)

**Goal**: Concurrent multi-URL processing

**Components**:
1. **Concurrency Control**: asyncio.Semaphore(5) to limit concurrent requests
2. **Progress Tracking**: tqdm progress bar or simple counter
3. **Error Aggregation**: Collect failed URLs with error details
4. **Deduplication**: Check Qdrant for existing URLs before processing

**Success Criteria**: Ingest 10 URLs with 3 intentional failures, verify 7 succeed and 3 logged correctly

---

### Phase 3: Configuration & Resilience (User Story 3 & 4 - P2/P3)

**Goal**: Configurable parameters and retry logic

**Components**:
1. **Config Validation**: Pydantic BaseModel for parameter validation
2. **Retry Logic**: tenacity decorator with exponential backoff
3. **Rate Limiting**: Respect Cohere 10 req/min limit
4. **Logging**: Structured logging (INFO for progress, ERROR for failures)

**Success Criteria**: Simulate rate limit (mock Cohere API), verify exponential backoff and eventual success

---

## Risk Analysis

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| Cohere rate limit exhaustion | High | Medium | Semaphore(5), exponential backoff, batch API usage |
| Qdrant free tier exceeded (1GB) | Low | High | Monitor storage, warn at 80% capacity |
| Large HTML page OOM | Medium | High | Stream parsing (BeautifulSoup), skip files >10MB |
| Malformed HTML breaks parser | Medium | Low | BeautifulSoup handles gracefully, log warnings |
| Network partition mid-batch | Low | Medium | Async timeout (30s), retry logic, log checkpoint |
| Embedding dimension mismatch | Low | Critical | Validate on startup, fail-fast if mismatch |

---

## Testing Strategy

### Unit Tests (pytest)

```python
@pytest.mark.asyncio
async def test_single_url_ingestion():
    result = await main(urls=["https://example.com"], verbose=False)
    assert result.success_count == 1
    assert result.total_chunks > 0

@pytest.mark.asyncio
async def test_invalid_url_handling():
    result = await main(urls=["https://invalid-12345.com"], verbose=False)
    assert result.failed_count == 1
    assert result.failed_urls[0].error_type == "http_error"

@pytest.mark.asyncio
async def test_duplicate_skipping():
    url = "https://example.com"
    result1 = await main(urls=[url], verbose=False)
    result2 = await main(urls=[url], skip_duplicates=True, verbose=False)
    assert result2.skipped_count == 1
```

### Integration Tests

```python
@pytest.mark.asyncio
async def test_end_to_end_with_real_apis():
    """Requires valid API keys in environment."""
    result = await main(
        urls=["https://docs.python.org/3/tutorial/introduction.html"],
        verbose=False
    )
    assert result.success_count == 1

    # Verify in Qdrant
    client = QdrantClient(url=os.getenv("QDRANT_URL"), api_key=os.getenv("QDRANT_API_KEY"))
    points = client.scroll("web_documents", limit=10)[0]
    assert len(points) >= result.total_chunks
```

---

## Deployment Guide

### Local Development

```bash
# Setup
git clone <repo>
cd <repo>
uv init
uv pip install -r backend/requirements.txt
cp backend/.env.example backend/.env
# Edit .env with API keys

# Run
cd backend
python main.py  # Or: python -m backend.main
```

### Production (Docker)

```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY backend/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY backend/main.py .
COPY backend/.env .
CMD ["python", "main.py"]
```

### FastAPI Integration

```python
# api.py
from fastapi import FastAPI
from backend.main import main

app = FastAPI()

@app.post("/api/ingest")
async def ingest_urls(request: IngestRequest):
    result = await main(urls=request.urls, verbose=False)
    return {"success_count": result.success_count, "total_chunks": result.total_chunks}
```

---

## Success Metrics

| Metric | Target | Measurement |
|--------|--------|-------------|
| Single URL latency | < 5s | Time from fetch to Qdrant storage |
| Batch success rate | ≥ 95% | Excluding intentional failures (404, timeout) |
| Embedding generation | < 2s/chunk | Cohere API latency |
| Qdrant queryability | Immediate | Zero-delay after upsert |
| Retry success rate | ≥ 90% | Transient failures recovered |
| Developer integration | < 30min | Quickstart completion time |
| Configuration changes | Zero code edits | Env vars + function params |
| Debug-friendliness | High | Logs sufficient for failure diagnosis |

---

## Appendix: Dependencies

```txt
# Core functionality
beautifulsoup4==4.12.2      # HTML parsing
aiohttp==3.9.1              # Async HTTP client
cohere==4.37.0              # Cohere SDK (embeddings)
qdrant-client==1.7.0        # Qdrant vector DB client
langchain==0.1.0            # Text splitters (chunking)
tiktoken==0.5.2             # Token counting

# Utilities
tenacity==8.2.3             # Retry logic with backoff
pydantic==2.5.3             # Config validation
python-dotenv==1.0.0        # .env file loading
tqdm==4.66.1                # Progress bars

# Testing (dev)
pytest==7.4.3               # Test framework
pytest-asyncio==0.21.1      # Async test support
```

**Installation via UV**:
```bash
uv pip install beautifulsoup4 aiohttp cohere qdrant-client langchain tiktoken tenacity pydantic python-dotenv tqdm
uv pip install --dev pytest pytest-asyncio
uv pip freeze > requirements.txt
```

---

## Next Steps

1. **Run `/sp.tasks`**: Break this plan into atomic implementation tasks
2. **Run `/sp.implement`**: Execute tasks to build `backend/main.py`
3. **Verify with quickstart**: Follow quickstart.md to test end-to-end
4. **Integrate with FastAPI**: Add REST endpoint wrapper (out of MVP scope)

---

**Plan Complete**: Ready for task generation (`/sp.tasks` command).
