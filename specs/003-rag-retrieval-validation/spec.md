# Feature Specification: RAG Retrieval Validation and Testing

**Feature Branch**: `003-rag-retrieval-validation`
**Created**: 2025-12-25
**Status**: Draft
**Input**: User description: "Retrieve stored embeddings and validate the RAG ingestion pipeline. Target audience: Backend developers verifying vector search functionality. Focus: Retrieving stored data from Qdrant and testing full ingestion-to-retrieval flow. Success criteria: Query embeddings using Qdrant's vector search API, Retrieve correct text chunks with metadata for given search queries, Validate pipeline correctness with sample inputs and test assertions, Confirm Cohere embeddings and Qdrant storage are consistent and functional. Constraints: Code remains in backend/main.py for now, Python + Cohere + Qdrant API, Minimal test queries for validation. Not building: Chatbot logic or response generation, Frontend connectivity or UI features"

## User Scenarios & Testing *(mandatory)*

### User Story 1 - Basic Vector Search Retrieval (Priority: P1)

As a backend developer, I want to query the Qdrant collection with a text query and retrieve relevant document chunks so that I can verify the vector search is working correctly.

**Why this priority**: This is the foundational capability that proves embeddings were stored correctly and semantic search is functional. Without this working, no RAG system can function.

**Independent Test**: Provide a natural language query (e.g., "How do I set up ROS 2 for humanoids?"), generate its embedding, search Qdrant, and verify that relevant chunks from Module 1 are returned with similarity scores above 0.7.

**Acceptance Scenarios**:

1. **Given** embeddings are stored in Qdrant for Module 1 (ROS 2 content), **When** a developer queries "ROS 2 fundamentals", **Then** the system returns the top 5 most relevant chunks from Module 1 documentation with similarity scores
2. **Given** a text query about NVIDIA Isaac, **When** the query is converted to an embedding and searched, **Then** results include chunks from Module 3 (Isaac-related content) ranked by relevance
3. **Given** embeddings exist for all 4 modules, **When** a developer searches for "autonomous humanoid capstone", **Then** Module 4 VLA-related chunks appear in the top results

---

### User Story 2 - Metadata Validation and Filtering (Priority: P2)

As a backend developer, I want to retrieve chunks with their full metadata (URL, title, chunk_index, timestamps) so that I can verify data integrity and filter results by source.

**Why this priority**: After basic retrieval works, developers need to ensure all metadata was preserved correctly during ingestion, which is critical for citation and traceability in RAG systems.

**Independent Test**: Query Qdrant with filters (e.g., only Module 2 URLs), retrieve results, and verify each chunk contains complete metadata fields (url, title, chunk_text, chunk_index, token_count, timestamp, model_name).

**Acceptance Scenarios**:

1. **Given** stored chunks with metadata, **When** a developer retrieves search results, **Then** each result includes url, title, chunk_text, chunk_index, and timestamp fields
2. **Given** documentation from multiple modules, **When** filtering by URL pattern (e.g., "module-1"), **Then** only chunks from Module 1 are returned
3. **Given** chunks with different token counts, **When** retrieving results, **Then** token_count metadata matches the actual tokenized length of chunk_text

---

### User Story 3 - Ingestion-to-Retrieval End-to-End Test (Priority: P2)

As a backend developer, I want to run a complete end-to-end test that ingests a sample URL, generates embeddings, stores them, and then retrieves them via search so that I can validate the entire pipeline is working correctly.

**Why this priority**: This provides confidence that the full ingestion-to-retrieval flow is functional and reproducible, which is essential before integrating with any chatbot or frontend.

**Independent Test**: Ingest a new test URL (e.g., a small documentation page), wait for completion, query for content from that URL, and verify it's retrievable with correct embeddings and metadata.

**Acceptance Scenarios**:

1. **Given** a test URL not yet in Qdrant, **When** the ingestion pipeline processes it and completes, **Then** a subsequent search query for content from that URL returns the newly ingested chunks
2. **Given** a test document with known content (e.g., "test document about robotics sensors"), **When** ingested and then searched with query "robotics sensors", **Then** the test document chunks appear in search results with high similarity
3. **Given** embeddings generated by Cohere for a test document, **When** querying Qdrant with the same text re-embedded, **Then** the original chunks are retrieved with similarity scores above 0.95 (near-identical match)

---

### User Story 4 - Consistency and Correctness Validation (Priority: P3)

As a backend developer, I want to run automated assertions that verify embedding dimensions, vector consistency, and metadata completeness so that I can catch data corruption or configuration issues early.

**Why this priority**: Automated validation provides confidence in data quality but is less critical than basic retrieval functionality. This is important for ongoing monitoring and debugging.

**Independent Test**: Run a validation script that checks: (1) all vectors have 1024 dimensions, (2) all chunks have required metadata fields, (3) embedding similarity between identical texts is >0.99, (4) no duplicate URLs unless intentionally re-ingested.

**Acceptance Scenarios**:

1. **Given** stored vectors in Qdrant, **When** validation checks dimension counts, **Then** all vectors have exactly 1024 dimensions (matching Cohere embed-english-v3.0)
2. **Given** chunks in the collection, **When** checking for required metadata, **Then** no chunks are missing url, chunk_text, or timestamp fields
3. **Given** the same text embedded twice, **When** comparing vector similarity, **Then** similarity score is >0.99 (vectors are nearly identical)
4. **Given** URLs already in Qdrant, **When** checking for unintentional duplicates, **Then** no duplicate URL entries exist unless skip_duplicates was set to False

---

### Edge Cases

- What happens when a query has no relevant results (similarity below threshold)?
- How does the system handle malformed queries (empty strings, special characters)?
- What if Qdrant collection is empty when querying?
- How are ties in similarity scores handled (multiple chunks with identical scores)?
- What happens when querying with a different embedding model than used for ingestion?
- How does filtering by non-existent metadata fields behave?
- What if chunk_text contains special characters or very long strings?

## Requirements *(mandatory)*

### Functional Requirements

- **FR-001**: System MUST generate query embeddings using the same Cohere model (embed-english-v3.0) and configuration used for document ingestion
- **FR-002**: System MUST query Qdrant collection using COSINE distance metric and return results ranked by similarity score
- **FR-003**: System MUST retrieve top N chunks (configurable, default 5) for a given text query
- **FR-004**: System MUST return complete metadata for each retrieved chunk including: url, title, chunk_text, chunk_index, token_count, timestamp, chunk_config, model_name, dimension
- **FR-005**: System MUST support filtering results by metadata fields (e.g., url pattern, timestamp range)
- **FR-006**: System MUST calculate and return similarity scores (0.0 to 1.0 range) for each retrieved chunk
- **FR-007**: System MUST validate that retrieved vectors match expected dimensions (1024 for Cohere embed-english-v3.0)
- **FR-008**: System MUST provide a programmatic interface for running retrieval queries (async function callable from Python)
- **FR-009**: System MUST include assertion-based validation tests that verify embedding consistency and metadata completeness
- **FR-010**: System MUST handle empty result sets gracefully (return empty list, not error)
- **FR-011**: System MUST log retrieval operations including query text, result count, and top similarity scores
- **FR-012**: System MUST support end-to-end testing workflow: ingest test document → query → verify retrieval
- **FR-013**: System MUST validate that Cohere API key and Qdrant credentials are functional before running queries
- **FR-014**: System MUST expose retrieval functionality through a simple test script demonstrating query examples
- **FR-015**: System MUST check for data consistency: no missing required metadata fields in retrieved chunks

### Key Entities

- **QueryRequest**: Represents a search query with attributes: query_text (input text), top_k (number of results), filters (optional metadata filters), include_metadata (boolean)
- **RetrievalResult**: Represents search results with attributes: chunks (list of retrieved chunks), query (original query text), total_results (count), execution_time (query duration)
- **RetrievedChunk**: Represents a single search result with attributes: chunk_id (Qdrant point ID), similarity_score (0.0-1.0), chunk_text (content), metadata (dict with url, title, chunk_index, etc.), vector (optional embedding for verification)
- **ValidationReport**: Represents validation test results with attributes: test_name, status (pass/fail), issues_found (list), total_checks, passed_checks, failed_checks

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: Developers can query the Qdrant collection and retrieve relevant results in under 1 second for typical queries
- **SC-002**: Vector search returns correct module-specific content with similarity scores above 0.7 for queries matching module topics
- **SC-003**: All retrieved chunks include complete metadata (100% of required fields populated)
- **SC-004**: End-to-end test (ingest → retrieve) completes successfully with test documents appearing in search results
- **SC-005**: Embedding consistency validation shows similarity above 0.99 for identical text queries
- **SC-006**: Automated validation tests pass with 100% success rate on expected checks (dimensions, metadata, consistency)
- **SC-007**: Developers can run retrieval tests independently without manual Qdrant or Cohere API interaction
- **SC-008**: Query filtering by URL pattern returns only chunks from specified sources with 100% accuracy

## Assumptions

- Cohere API credentials and Qdrant connection details are already configured in environment variables (from previous ingestion work)
- Qdrant collection "web_documents" exists and contains embeddings from the 4-module documentation ingestion
- Embedding model (embed-english-v3.0) and dimensions (1024) remain consistent between ingestion and retrieval
- Developers have basic Python and async/await knowledge
- Test queries will be run against already-ingested documentation (Module 1-4 content)
- Validation tests are for development/testing only, not production monitoring

## Dependencies

- **Cohere API**: Required for generating query embeddings (must use same model as ingestion)
- **Qdrant**: Vector database with existing "web_documents" collection
- **Python libraries**: cohere (async client), qdrant-client, python-dotenv (already installed from ingestion pipeline)
- **Existing ingestion pipeline**: Must be functional and have ingested documentation before retrieval can be tested

## Constraints

- Code must remain in backend/main.py for now (no separate modules or files)
- Python + Cohere + Qdrant API only (no additional frameworks)
- Minimal test queries for validation (not comprehensive test suite)
- No chatbot logic, response generation, or frontend connectivity
- Retrieval functions must be compatible with existing ingestion code structure
- Must use async/await patterns consistent with existing pipeline

## Out of Scope

- **Chatbot logic**: Generating conversational responses from retrieved chunks
- **Response generation**: Formatting or synthesizing answers from search results
- **Frontend integration**: Web UI, API endpoints, or user-facing interfaces
- **Advanced RAG features**: Re-ranking, context windowing, multi-query retrieval
- **Production monitoring**: Real-time alerting, performance dashboards, or SLA tracking
- **Comprehensive testing**: Full test suite with edge case coverage (minimal validation only)
- **Query optimization**: Query rewriting, semantic expansion, or relevance tuning
- **Result formatting**: Markdown rendering, citation formatting, or presentation logic
